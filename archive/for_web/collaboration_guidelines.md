# Human-LLM Collaboration Guidelines

## Overview

This document provides practical guidelines for human-LLM collaboration in the game generation workflow. It focuses particularly on feedback collection, question management, and best practices.

**Target Audience:**

- LLM Agents: Learn effective communication methods with humans
- Human Operators: Understand how to provide effective feedback

**Related Documents:**

- `game_generation_workflow.md`: Complete details of the integrated workflow
- `one-button-game-design-guide.md`, `one-button-game-implementation-guide.md`: Original collaborative approaches

---

## Feedback Collection Templates

Collection of feedback templates for each phase.

### Phase 0: Tag Selection and Initial Validation

**Purpose:** Validate the feasibility and appeal of tag combinations

**Template:**

```markdown
ã€Tag Selection Feedback Requestã€‘

Please verify the feasibility of the selected tag combination:

ã€Selection Resultsã€‘

- Tags: [list]
- Categories: [number] ([category names])
- Dominance rate: [max category] [percentage]%
- Novelty score: [value] (tag pair novelty rate)

ã€Questionsã€‘

1. Is this tag combination appealing?

   - Can you imagine any interesting interactions?
   - Are there any awkward combinations?

2. Is the category balance appropriate?

   - Do any specific categories feel lacking/excessive?

3. What problem category is expected from these tags?
   - Movement/Navigation
   - Resource/Collection
   - Timing/Coordination
   - Information/Visibility
   - State/Balance
   - Physics/Forces
   - Pattern/Signal

ã€Response Optionsã€‘
âœ… "Approved, proceed" + indicate expected problem category
ğŸ”„ "Change tag X to tag Y" â†’ reselection
âŒ "Overall appeal is weak" â†’ complete reselection
```

**Expected Responses:**

```markdown
âœ… Good: "Approved. Physics/Forces problem expected"
âœ… Good: "Approved, proceed. Timing/Coordination looks interesting"
ğŸ”„ Adjustment: "Change weapon:explosion to weapon:beam"
ğŸ”„ Adjustment: "Lacking player tags. Add player:rotate"
âŒ Reject: "Combination is bland. Complete reselection"
```

---

### Phase 1: Problem-Solution Structuring

**Purpose:** Validate the feasibility and logic of problem-solution logic

**Template:**

```markdown
ã€Problem-Solution Logic Validation Requestã€‘

Please verify the feasibility of the problem-solution logic:

ã€Problem Definitionã€‘

- Player wants to: [specific goal]
- Current obstacle: [what hinders]
- Environmental constraint: [why normal means don't work]

Is this problem definition clear and appealing?

ã€Solutionã€‘

- Baseline verb: "[base verb]"
- Light improvement: "[light twist]"
- Control:
  - Press: [immediate action]
  - Hold: [continuous action/parameter change]
  - Release: [action execution/state change]

Is this solution intuitive? Can it be understood within 3 seconds?

ã€Logic Chainã€‘
Problem ([obstacle])
â†“
Solution ([solution])
â†“
Goal ([goal])

Does this logical flow feel natural? Are there any contradictions?

ã€Response Optionsã€‘
âœ… "Approved, logical and interesting"
ğŸ”„ "Adjust problem definition: [specific suggestion]"
ğŸ”„ "Adjust solution: [specific suggestion]"
âŒ "Fundamental contradiction: [reason]" â†’ Redesign Phase 1
```

**Expected Responses:**

```markdown
âœ… Good: "Approved, logical and interesting"
âœ… Good: "Simple and clear, proceed"
ğŸ”„ Adjustment: "Problem definition: 'place' is more accurate than 'collect objects'"
ğŸ”„ Adjustment: "Solution: timing explosion is more intuitive than charged explosion"
âŒ Reject: "Contradictory that explosion can push back against gravity" â†’ redesign
```

---

### Phase 2: Creative Synthesis and Novelty Assurance

**Purpose:** Validate creativity, differentiation, and implementability

**Template:**

```markdown
ã€Creativity and Implementability Validation Requestã€‘

Please verify creativity and implementability:

ã€Concept Evaluationã€‘

- Does this game concept feel fresh and appealing?
- Is the difference from existing games clear?
- Would you want to play this?

ã€Differentiation Elementsã€‘
Explicitly excluded:

- Victory condition: "[new condition]" (not existing "[old condition]")
- Control method: "[new method]" (not existing "[old method]")
- Game loop: "[new loop]" (not existing "[old loop]")

Do these differentiations feel sufficient?

ã€Implementabilityã€‘
Please review the logical walkthrough:
[step-by-step flow]

Are there any impossibilities or contradictions in this flow?

ã€Visual Communicationã€‘
Are the presented visual feedback specifications appropriate?

- Ease of problem recognition
- Immediate action feedback
- Clarity of progress

ã€Engagement/Innovation Judgmentã€‘
ğŸ¤ Please provide your intuitive judgment:

- Can this mechanic create strategic depth?
- Is there a risk of simple repetition?
- Does this "light twist" contribute to essential fun?
- Is it just a superficial gimmick?

ã€Response Optionsã€‘
âœ… "Approved, implementable and appealing"
ğŸ”„ "Adjust concept: [specific suggestion]"
ğŸ”„ "Add differentiation element: [specific suggestion]"
ğŸ”„ "Adjust visual feedback: [specific suggestion]"
âŒ "Fundamental issue: [reason]" â†’ Return to Phase 1
```

**Expected Responses:**

```markdown
âœ… Good: "Approved, implementable and appealing"
âœ… Good: "Clear differentiation, has strategic depth"
ğŸ”„ Adjustment: "Differentiation element: want visual representation difference too (colors etc.)"
ğŸ”„ Adjustment: "Visual feedback: emphasize charge display more"
âŒ Reject: "Ends up being the same as existing gravity games" â†’ Return to Phase 1
```

---

### Phase 3: Implementation and Prototyping

**Purpose:** Verify basic functionality and parameter tuning

**Template:**

```markdown
ã€Basic Functionality Verification Requestã€‘

Please verify basic functionality:

ã€Comprehensibility Testã€‘

- Can you understand the game's objective within 3 seconds?
- Is what you should do clear?

ã€Reachability Testã€‘

- Can you reach the goal with only button operations?
- Are there any unnatural or impossible operations?

ã€Control Feel Testã€‘

- Is button press response immediate and clear?
- Is the hold (charge) effect easy to understand?
- Is the release (explosion) feedback appropriate?

ã€Visual Feedback Testã€‘
Are the visual communication specifications designed in Phase 2 implemented?

- Problem recognition (danger zone, safe zone indication)
- Action feedback (explosion effects, charge display)
- Progress display (number of saved objects)
- Failure warning (warning when approaching center)

ã€Specific Feedback Examplesã€‘
âœ… "Understood, interesting"
ğŸ”„ "Movement is too slow" â†’ parameter adjustment
ğŸ”„ "Explosion is too weak" â†’ parameter adjustment
ğŸ”„ "Charge display is hard to see" â†’ visual adjustment
âŒ "Cannot reach goal" â†’ logic review

Please provide specific feedback.
```

**Expected Responses:**

```markdown
âœ… Good: "Understood, interesting. Proceed"
âœ… Good: "Control feel is good, visuals are clear"
ğŸ”„ Parameter: "Movement is too slow and boring"
â†’ LLM: "Adjust GRAVITY_STRENGTH 0.1 â†’ 0.15"
ğŸ”„ Parameter: "Explosion is too weak"
â†’ LLM: "Adjust EXPLOSION_FORCE 2 â†’ 2.5"
ğŸ”„ Visual: "Charge circle is hard to see"
â†’ LLM: "color('light_cyan') â†’ color('cyan'), line width 2â†’3"
âŒ Logic: "Gravity is working in the wrong direction"
â†’ Logic fix
```

---

### Phase 4: Validation and Balance Adjustment

**Purpose:** Balance judgment and parameter feel verification

**Template (4-1: Diagnosis Result Judgment):**

```markdown
ã€GA Diagnosis Result Judgment Requestã€‘

Please review the GA diagnosis results:

ã€Diagnosis Summaryã€‘

- GA Best Score: [value]
- Monotonous Best: [value] ([pattern name])
- Normalized GA score: [value] ([percentage]%)
- Normalized monotonous score: [value] ([percentage]%)
- GA Resistance: [Low/Moderate/High] ([score] points)

ã€Judgment Resultã€‘
Normalized monotonous score [â‰¤/>] 0.5: [âœ…/âŒ] [interpretation]
Normalized GA score [value]: [judgment]

ã€Questionsã€‘

1. Is this vulnerability level ([Low/Moderate/High]) acceptable?

   - Is a [multiplier]x score difference with skill-based play sufficient?

2. Should we perform balance adjustment?

   - Target score: What is appropriate? (recommended: 100-150)
   - Target survival time: How many seconds? (recommended: 30-60 seconds)

3. Or should we review the game design?
   - Return to Phase 2 to strengthen differentiation elements?

ã€Response Optionsã€‘
âœ… "Acceptable range, proceed to Phase 5"
ğŸ”§ "Perform balance adjustment: target score [value], survival time [value] seconds"
ğŸ”„ "Review design: [specific improvement proposal]" â†’ Return to Phase 2
```

**Template (4-2: Post-Adjustment Verification):**

```markdown
ã€Post-Balance Adjustment Verification Requestã€‘

Please test the game after balance adjustment:

ã€Core Mechanic Understandingã€‘

- Can you understand the unique system ([mechanic name]) operation?
- Are the mechanic interactions interesting?

ã€Difficulty Evaluationã€‘

- Is the difficulty appropriate? (too easy/too difficult)
- Is the learning curve natural?

ã€Parameter Feelã€‘
Verify the feel of adjusted parameters:

- Is [parameter 1] appropriate? (too fast/too slow)
- Is [parameter 2] appropriate? (too strong/too weak)
- Is [parameter 3] appropriate? (too fast/too slow)

ã€Risk-Reward Balanceã€‘

- Does taking risks feel worthwhile?
- Is the balance between safe and adventurous strategies appealing?

ã€Specific Feedback Examplesã€‘
âœ… "Perfect, proceed"
ğŸ”„ "Still [parameter] is [too strong/weak/fast/slow]" â†’ readjust
âŒ "Too monotonous" â†’ Return to Phase 2

We'll iterate adjustments until satisfied. Please provide specific feedback.
```

**Expected Responses:**

```markdown
ã€Diagnosis Result Judgmentã€‘
âœ… Good: "Acceptable range, proceed to Phase 5"
âœ… Good: "2.5x score difference is sufficient for skill-based"
ğŸ”§ Balance: "Perform balance adjustment: target score 100, survival time 45 seconds"
ğŸ”§ Balance: "High scores with HoldOnly is problematic. Please adjust"
ğŸ”„ Redesign: "Mechanic itself is monotonous. Review in Phase 2"

ã€Post-Adjustment Verificationã€‘
âœ… Good: "Perfect, proceed"
âœ… Good: "Good balance, interesting"
ğŸ”„ Parameter: "Gravity is still too strong"
â†’ LLM: "GRAVITY_STRENGTH 0.12 â†’ 0.10"
ğŸ”„ Parameter: "Charge is slow"
â†’ LLM: "CHARGE_RATE 0.5 â†’ 0.7"
âŒ Depth: "No strategic depth, monotonous"
â†’ Return to Phase 2
```

---

### Phase 5: Final Validation and Completion Approval

**Purpose:** Final experience evaluation and completion judgment

**Template:**

```markdown
ã€Final Experience Evaluation Requestã€‘

Please provide final experience evaluation:

ã€Replay Motivationã€‘

- Do you want to repeatedly play this game?
- Does it create a "one more time" feeling?
- Can you feel improvement after playing several times?

ã€Remaining Issues Checkã€‘

- Are there any remaining issues with balance or controls?
- Are there any visually unclear parts?
- Are game over conditions fair and clear?

ã€Completion Evaluationã€‘

- Can this state be considered "complete"?
- Has it reached a level where others can enjoy it?

ã€Comprehensive Report Reviewã€‘
Please review the presented comprehensive evaluation report:

- Overall Score: [value]/100
- Are each score reasonable?
- Are differentiation elements clear?

ã€Response Optionsã€‘
âœ… "Approved/Complete" â†’ Record metrics, save logs
ğŸ”„ "Minor adjustment: [specific content]" â†’ Return to Phase 4
âŒ "Fundamental issue: [reason]" â†’ Return to Phase [number] (human specifies)

Please provide final judgment.
```

**Expected Responses:**

```markdown
âœ… Good: "Approved/Complete"
âœ… Good: "Replay motivation present, completion sufficient"
ğŸ”„ Minor: "Minor adjustment: emphasize charge sound more"
â†’ LLM: "Adjust play('select') volume"
â†’ Re-present
ğŸ”„ Minor: "Move progress display to top"
â†’ LLM: "Adjust text() coordinates"
â†’ Re-present
âŒ Major: "Too similar to existing games after all"
â†’ Return to Phase 2
âŒ Major: "Problem-solution logic has contradictions"
â†’ Return to Phase 1
```

---

## Question Management Best Practices

### Level 1: Blocker Question (Implementation-Stopping Question)

**When to Use:**

- Fundamental implementation direction is unclear
- Can be clarified with Yes/No or 2-3 choices
- Implementation is impossible without this judgment

**Good Examples:**

```markdown
âœ… "Confirmation: When colliding with enemies, does it immediately game over?
Or does health decrease?
(This judgment changes the game loop design)"

âœ… "Confirmation: Is the safe zone a fixed position on screen?
Or does it move dynamically?
(This judgment changes the explosion timing design)"

âœ… "Confirmation: Does the player control multiple objects simultaneously?
Or only one object?
(This judgment changes the state management design)"
```

**Bad Examples:**

```markdown
âŒ "Please explain enemy behavior in detail"
â†’ Too broad. Should break into specific questions

âŒ "What is this game's concept?"
â†’ Should already be defined in Phase 1-2

âŒ "What score is good?"
â†’ Should be treated as Level 3 (Parameter Question)
```

**Response Pattern:**

```markdown
ğŸ¤– Question: [clear question]
ğŸ¤ Human Answer: [Yes/No/choice]
ğŸ¤– Acknowledgment: "Understood. Will proceed with design as [decision content]"
```

---

### Level 2: Assumption Clarification

**When to Use:**

- Reasonable default values exist for implementation
- Typical patterns can be inferred from tags and categories
- Implementation is possible with human approval

**Good Examples:**

```markdown
âœ… "Assumption: Player warps to opposite side when reaching screen edge

Reason: Tag 'field:wrap' is selected, which is a typical implementation
pattern in Crisp Game Library.
May I proceed with this assumption?"

âœ… "Assumption: Use 5 objects

Reason: In existing games with player:multiple tag (knowledge/tag_code_map.json),
3-7 objects is the standard range. Starting with mid-value of 5.
Is this assumption acceptable?"

âœ… "Assumption: Explosion effect uses particle() function

Reason: Common pattern in existing implementations with weapon:explosion tag.
May I proceed with this assumption?"
```

**Bad Examples:**

```markdown
âŒ "Assumption: Game will be fun"
â†’ Too vague. Not a specific implementation decision

âŒ "Assumption: Score is 100 points"
â†’ Should be treated as Level 3. Parameter, not assumption

âŒ "Assumption: This game has high novelty"
â†’ Evaluation, not decision. Should verify with similarity check
```

**Response Pattern:**

```markdown
ğŸ¤– Assumption: [specific assumption]
Reason: [rationale]
Please approve.

ğŸ¤ Human Response:
âœ… "Approved" â†’ continue implementation
ğŸ”„ "Change: [specific content]" â†’ follow instructions
âŒ "Inappropriate" â†’ present alternatives
```

---

### Level 3: Parameter Question

**Basic Policy:**

- **Don't ask before implementing**
- Implement with standard values â†’ present â†’ collect feedback
- Wait for specific adjustment instructions based on human experience

**Good Flow:**

```markdown
ğŸ¤– "Implemented with MOVE_SPEED = 1.0. Please test"
ğŸ¤ "Movement is too slow"
ğŸ¤– "Will adjust MOVE_SPEED to 1.5"
[Code update]
ğŸ¤– "Adjustment complete. Please retest"
ğŸ¤ "Improved. Proceed"
```

**Bad Flow:**

```markdown
âŒ ğŸ¤– "Which is better for MOVE_SPEED: 0.5, 1.0, or 1.5?"
â†’ Burdens human. Should ask after implementing

âŒ ğŸ¤– "Please tell me optimal MOVE_SPEED"
â†’ Abstract. Implement â†’ experience â†’ feedback is correct

âŒ ğŸ¤– "Will implement with MOVE_SPEED = 2.0, any issues?"
â†’ No need for pre-confirmation. Implement and let them experience
```

**Parameter Categories:**

```markdown
ã€Physical Parametersã€‘

- Speed: MOVE_SPEED, ROTATION_SPEED
- Gravity: GRAVITY_STRENGTH
- Jump: JUMP_POWER
  â†’ Implement with default values, adjust after experience

ã€Timing Parametersã€‘

- Intervals: SPAWN_INTERVAL, ATTACK_INTERVAL
- Charge: CHARGE_RATE, MAX_CHARGE_TIME
  â†’ Implement with mid-values, adjust after experience

ã€Balance Parametersã€‘

- Quantities: OBJECT_COUNT, ENEMY_COUNT
- Ranges: EXPLOSION_RADIUS, SAFE_ZONE_RADIUS
  â†’ Implement with typical values, adjust after experience
```

---

## Communication Patterns

### Effective Feedback Solicitation

**LLM Side Best Practices:**

```markdown
ã€Good Request Methodsã€‘
âœ… "Phase 3 implementation complete. Please verify basic functionality.
Please check especially the following points:

- Can objective be understood within 3 seconds
- Can goal be reached with button operations only
- Is operation response immediate and clear

Please provide specific feedback."

âœ… "Parameter adjustment complete. Please retest.
Regarding previous feedback (movement too slow), changed MOVE_SPEED 1.0â†’1.5.
Please confirm improvement."

ã€Bad Request Methodsã€‘
âŒ "How is it?"
â†’ Unclear what to evaluate

âŒ "Perfect, right?"
â†’ Leading. Hinders objective evaluation

âŒ "I think there's no problem, but please check anyway"
â†’ Passive. Doesn't encourage serious evaluation
```

### Interpreting Human Feedback

**Feedback Interpretation Patterns:**

```markdown
ã€Clear Feedbackã€‘
âœ… "Movement is too slow"
â†’ Increase MOVE_SPEED (e.g., 1.0 â†’ 1.5)

âœ… "Explosion is too weak"
â†’ Increase EXPLOSION_FORCE (e.g., 2 â†’ 2.5)

âœ… "Charge is too fast"
â†’ Decrease CHARGE_RATE (e.g., 1.0 â†’ 0.7)

ã€Ambiguous Feedbackã€‘
âš ï¸ "Something's off"
â†’ ğŸ¤– Clarification question: "Is it speed/timing/difficulty?"

âš ï¸ "Not quite right"
â†’ ğŸ¤– Clarification question: "At what point do you feel issues: first 10 seconds/mid-game/end-game?"

âš ï¸ "Not fun"
â†’ ğŸ¤– Clarification question: "Is the mechanic monotonous, or is it a difficulty issue?"

ã€Gradual Narrowingã€‘
ğŸ¤– "First, is control feel okay? (Yes/No)"
ğŸ¤ "Yes"
ğŸ¤– "Is difficulty appropriate? (Yes/No)"
ğŸ¤ "No, too difficult"
ğŸ¤– "Understood. Will reduce difficulty"
```

**Determining Adjustment Direction:**

```markdown
ã€Typical Feedback â†’ Adjustment Mappingã€‘

"Too fast" â†’ Reduce parameter to 70-80%
"Too slow" â†’ Increase parameter to 120-150%
"Too strong" â†’ Reduce parameter to 70-80%
"Too weak" â†’ Increase parameter to 120-150%

"A bit fast" â†’ Reduce parameter to 85-90%
"A bit slow" â†’ Increase parameter to 110-120%

"Very fast" â†’ Reduce parameter to 50-60%
"Very slow" â†’ Increase parameter to 150-200%

ã€Adjustment Range Principlesã€‘

- First adjustment: Â±20-50% (bold)
- Second adjustment: Â±10-20% (moderate)
- Third+ adjustment: Â±5-10% (fine-tuning)
```

---

## Best Practices

### For LLM Agents

**DO (Recommended Actions):**

```markdown
âœ… Strictly adhere to completion criteria for each phase

- Don't proceed until obtaining clear human approval

âœ… Immediately detect and resolve ambiguities

- Level 1 Blocker: Stop implementation and ask
- Level 2 Assumption: State assumption and wait for approval
- Level 3 Parameter: Implement first, then let them experience

âœ… Request feedback specifically

- State evaluation points
- Present specific answer examples

âœ… Keep adjustment cycles short

- Change only one parameter per adjustment
- Rapid iteration of: adjust â†’ test â†’ feedback

âœ… Visualize progress

- Clearly mark start/completion of each phase
- Record adjustment history
```

**DON'T (Actions to Avoid):**

```markdown
âŒ Present multiple options and make humans choose

- "Which is better: Plan A or Plan B?"
- â†’ Correct: single implementation â†’ feedback â†’ adjustment

âŒ Implement after making assumptions, then report

- "Implemented based on this judgment"
- â†’ State assumptions beforehand and get approval

âŒ Accept ambiguous feedback as-is

- "Something's off" â†’ adjust without clarification questions
- â†’ Gradually narrow down

âŒ Proceed to next phase without meeting completion criteria

- "Seems about right, so on to Phase 4"
- â†’ Wait for clear human approval

âŒ Change multiple parameters at once

- Adjust MOVE_SPEED, JUMP_POWER, GRAVITY simultaneously
- â†’ Unclear which change was effective
```

---

### For Human Operators

**DO (Recommended Actions):**

```markdown
âœ… Provide specific feedback

- Good: "Movement is too slow"
- Bad: "Something's off"

âœ… Focus on evaluation points for each phase

- Phase 0: Tag appeal
- Phase 1: Logic feasibility
- Phase 2: Creativity and differentiation
- Phase 3: Basic functionality
- Phase 4: Balance
- Phase 5: Completion

âœ… Point out problems early

- If logical contradictions found in Phase 1, point out immediately
- Don't drag to Phase 5

âœ… Clear approval/rejection

- Good: "Approved, proceed"
- Good: "Rejected, [reason]"
- Bad: "Well, I guess it's okay"
```

**DON'T (Actions to Avoid):**

```markdown
âŒ Ambiguous feedback

- "Something's off", "Not quite right", "So-so"
- â†’ LLM spends time on narrowing questions

âŒ Point out multiple problems simultaneously

- "Speed, timing, and difficulty are all problems"
- â†’ Adjust one at a time in order

âŒ Postpone problems

- Approve despite feeling off in Phase 1
- â†’ Major rework occurs in Phase 4

âŒ Rejection without reason

- Just "No"
- â†’ LLM cannot determine improvement direction
```

---

## Troubleshooting Common Issues

### Issue 1: Prolonged Feedback Cycles

**Symptom:** 10+ adjustment iterations in the same phase

**Cause Analysis:**

```markdown
1. Parameter adjustment range too small
   â†’ 5% adjustments each time slow convergence

2. Human feedback is ambiguous
   â†’ "Something's off" doesn't indicate direction

3. Simultaneous adjustment of multiple parameters
   â†’ Unclear which change was effective
```

**Solution:**

```markdown
ğŸ¤– LLM Side:

- First adjustment should be bold (Â±30-50%)
- Actively use clarification questions
- Show adjustment history to display trends

ğŸ¤ Human Side:

- Point out specific issues
- Relative evaluation: "better/worse than before"
- Approve immediately when satisfied
```

---

### Issue 2: Frequent Rework

**Symptom:** Major rework like Phase 4â†’Phase 1, Phase 5â†’Phase 2

**Cause Analysis:**

```markdown
1. Insufficient validation in early phases
   â†’ Missed logical contradictions in Phase 1

2. Loose application of completion criteria
   â†’ Proceeded with "about OK"

3. Overlooked warning signs
   â†’ Ignored Red Flags
```

**Solution:**

```markdown
ğŸ¤– LLM Side:

- Strictly apply completion criteria
- Don't overlook warning signs
- Emphasize "final confirmation before implementation" at Phase 2 completion

ğŸ¤ Human Side:

- Strict evaluation in Phase 1/2
- Point out immediately if something feels off
- Be careful with "approval"
```

---

### Issue 3: Repeated Rejections in Similarity Check

**Symptom:** 70%+ similarity for 3+ consecutive times in Phase 3

**Cause Analysis:**

```markdown
1. Insufficient differentiation in Phase 2
   â†’ Exclusion filtering is weak

2. Tag selection issues
   â†’ Same tag combination as existing games

3. Only superficial changes
   â†’ Changed victory condition but same control method
```

**Solution:**

```markdown
ğŸ¤– LLM Side:

- Strengthen exclusion filtering in Phase 2
- Multi-level differentiation (victory condition + control + visual)
- Propose returning to Phase 0 after 3 rejections

ğŸ¤ Human Side:

- Suggest creative breakthroughs in Phase 2
- Ideas for "interesting uses not in existing games"
- Judgment on tag reselection
```

---

## Templates Summary

| Phase | Template Name                       | Purpose                        | Key Questions                                    |
| ----- | ----------------------------------- | ------------------------------ | ------------------------------------------------ |
| 0     | Tag Selection Feedback              | Tag feasibility validation     | Appealing? Balanced? Expected problem?           |
| 1     | Problem-Solution Logic Validation   | Logic validation               | Clear? Intuitive? Any contradictions?            |
| 2     | Creativity & Implementability Check | Differentiation, feasibility   | Fresh? Differentiated? Any impossibilities?      |
| 3     | Basic Functionality Verification    | Basic operation, control feel  | Understandable? Reachable? Response?             |
| 4-1   | GA Diagnosis Result Judgment        | Balance judgment               | Acceptable range? Adjust?                        |
| 4-2   | Post-Adjustment Verification        | Parameter feel                 | Understandable? Difficulty? Feel?                |
| 5     | Final Experience Evaluation         | Completion judgment            | Want to replay? Complete?                        |
